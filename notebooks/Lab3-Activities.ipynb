{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T10:45:07.977718Z",
     "iopub.status.busy": "2023-05-26T10:45:07.977235Z",
     "iopub.status.idle": "2023-05-26T10:45:10.739675Z",
     "shell.execute_reply": "2023-05-26T10:45:10.738872Z",
     "shell.execute_reply.started": "2023-05-26T10:45:07.977643Z"
    },
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1654594114799,
     "user": {
      "displayName": "Francisco Melo",
      "userId": "00306568718420504230"
     },
     "user_tz": -60
    },
    "id": "aWvGUgnDabis",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import shuffle, choice\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as datautils\n",
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x42z0hX-Cr-o"
   },
   "source": [
    "# Activities\n",
    "\n",
    "1. How many parameters does the first recurrent neural network have? And the LSTM network?\n",
    "\n",
    "2. Why are exploding/vanishing gradients a more pressing issue in recurrent neural networks? And how does the LSTM architecture help to address the vanishing gradient problem?\n",
    "\n",
    "3. Repeat the training of the LSTM network, but now using all the words in the two datasets (training and test) as training data. Let the network train for 40 epochs. Then, for each of the 4 possible labels/languages, generate 5 sample names using the trained network. Comment on the performance of the network thus trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8jx134dakEm",
    "tags": []
   },
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T10:45:10.740753Z",
     "iopub.status.busy": "2023-05-26T10:45:10.740461Z",
     "iopub.status.idle": "2023-05-26T10:45:10.746775Z",
     "shell.execute_reply": "2023-05-26T10:45:10.745440Z",
     "shell.execute_reply.started": "2023-05-26T10:45:10.740735Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1654594115088,
     "user": {
      "displayName": "Francisco Melo",
      "userId": "00306568718420504230"
     },
     "user_tz": -60
    },
    "id": "NcyrGU80-WhJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(net, dataset, loss):\n",
    "  \n",
    "    # Compute dataset size\n",
    "    dataset_size = len(dataset)\n",
    "\n",
    "    # We first set the network to \"evaluation mode\". This is useful, for \n",
    "    # example, in dropout layers, which should behave differently in training \n",
    "    # and evaluation.\n",
    "    net.eval()\n",
    "\n",
    "    l = 0\n",
    "    acc = 0\n",
    "\n",
    "    # We compute both scores and labels\n",
    "    for X, y in dataset:\n",
    "        output = net(X)\n",
    "        _, label = torch.max(output, dim=1)\n",
    "\n",
    "        # Compute loss\n",
    "        l += loss(output, y).item()\n",
    "            \n",
    "        # Compute accuracy\n",
    "        acc += (label == y).double().item()\n",
    "\n",
    "    # Average\n",
    "    l = l / dataset_size\n",
    "    acc = acc / dataset_size\n",
    "    \n",
    "    # We reset the network back to training mode\n",
    "    net.train()\n",
    "    \n",
    "    return l, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T10:45:10.749861Z",
     "iopub.status.busy": "2023-05-26T10:45:10.748808Z",
     "iopub.status.idle": "2023-05-26T10:45:10.759904Z",
     "shell.execute_reply": "2023-05-26T10:45:10.758490Z",
     "shell.execute_reply.started": "2023-05-26T10:45:10.749793Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1654594115089,
     "user": {
      "displayName": "Francisco Melo",
      "userId": "00306568718420504230"
     },
     "user_tz": -60
    },
    "id": "8ZQERgBtUtqd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_network(net, loss, optimizer, dataset, num_epochs=20):\n",
    "    \n",
    "    # We start by initializing two lists, to track the loss and accuracy during\n",
    "    # training.\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    valid_losses = []\n",
    "    valid_accuracies = []\n",
    "\n",
    "    for ep in range(num_epochs):\n",
    "        print('\\n- Training epoch: %i -' % ep)\n",
    "\n",
    "        # We use auxiliary variables to keep track of loss and accuracy within \n",
    "        # an epoch\n",
    "        running_loss = 0.\n",
    "        running_acc  = 0.\n",
    "\n",
    "        for X, y in dataset:\n",
    "\n",
    "            # We zero-out the gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute output\n",
    "            output = net(X)\n",
    "\n",
    "            # Our outputs are *scores*, so we also compute the predicted labels, \n",
    "            # since we need them to check the accuracy\n",
    "            #\n",
    "            # To that purpose, we compute the class that maximizes the score. \n",
    "            # The max function returns both the maximum value, and the \n",
    "            # maximizing entry. We care only about the latter, so we ignore the \n",
    "            # first output. \n",
    "            #\n",
    "            # Also, recall that the dimensions of the output are \n",
    "            # (batch size, n. classes). We take the maximum over the first \n",
    "            # dimension\n",
    "            _, label = torch.max(output, dim=1)\n",
    "\n",
    "            # Get loss\n",
    "            l = loss(output, y)\n",
    "\n",
    "            # Compute gradient\n",
    "            l.backward()\n",
    "            \n",
    "            # Perform optimization step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update total running loss. We account for the number of points \n",
    "            # in the batch\n",
    "            running_loss += l.item()\n",
    "             \n",
    "            # Update the accuracy\n",
    "            running_acc += (label == y.data).double().item()\n",
    "\n",
    "        train_losses += [running_loss / len(dataset)]\n",
    "        train_accuracies += [running_acc / len(dataset)]\n",
    "\n",
    "        # Loss and accuracy in the validation set\n",
    "        aux_l, aux_a = evaluate(net, valid_set, loss)\n",
    "\n",
    "        valid_losses += [aux_l]\n",
    "        valid_accuracies += [aux_a]\n",
    "\n",
    "        print(f'Training loss: {train_losses[-1]:.4f}')\n",
    "        print(f'Training accuracy: {train_accuracies[-1]:.1%}')\n",
    "        print(f'Validation loss: {valid_losses[-1]:.4f}')\n",
    "        print(f'Validation accuracy: {valid_accuracies[-1]:.1%}')\n",
    "\n",
    "    return net, train_losses, train_accuracies, valid_losses, valid_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T10:45:10.764302Z",
     "iopub.status.busy": "2023-05-26T10:45:10.763918Z",
     "iopub.status.idle": "2023-05-26T10:45:10.773677Z",
     "shell.execute_reply": "2023-05-26T10:45:10.772789Z",
     "shell.execute_reply.started": "2023-05-26T10:45:10.764257Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1654594115089,
     "user": {
      "displayName": "Francisco Melo",
      "userId": "00306568718420504230"
     },
     "user_tz": -60
    },
    "id": "K9SCqr-7rMNq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "LABELS = ['English', 'French', 'Portuguese', 'Spanish']\n",
    "LETTERS = list(string.ascii_letters + \" .,;'-\") + ['<eos>']\n",
    "\n",
    "def input_encoding(input_str):\n",
    "    ''' Receives a string as input and returns, as output, a Pytorch tensor\n",
    "        containing the one-hot encoding of the provided string.'''\n",
    "\n",
    "    one_hot_string = torch.zeros(len(input_str), 1, len(LETTERS), dtype=torch.float)\n",
    "    \n",
    "    for letter_idx in range(len(input_str)):\n",
    "        letter = input_str[letter_idx]\n",
    "        one_hot_string[letter_idx][0][LETTERS.index(letter)] = 1\n",
    "\n",
    "    return one_hot_string\n",
    "\n",
    "def label_encoding(output_str):\n",
    "    ''' Receives a string as input and returns, as output, a Pytorch tensor\n",
    "        containing the one-hot encoding of the provided label string.'''\n",
    "\n",
    "    one_hot_label = torch.zeros(1, len(LABELS), dtype=torch.float)\n",
    "    label_idx = LABELS.index(output_str)\n",
    "    one_hot_label[0][label_idx] = 1\n",
    "\n",
    "    return one_hot_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fufhIyctBcCP"
   },
   "source": [
    "We are now going to create a custom dataset. We create a subclass of Pytorch's `Dataset` class, and write down the `__init__`, `__len__`, and `__getitem__` methods. \n",
    "\n",
    "We thus load the file `names_data.csv` that we just uploaded and then use the data in it to create the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T10:45:10.776038Z",
     "iopub.status.busy": "2023-05-26T10:45:10.775469Z",
     "iopub.status.idle": "2023-05-26T10:45:10.785088Z",
     "shell.execute_reply": "2023-05-26T10:45:10.783419Z",
     "shell.execute_reply.started": "2023-05-26T10:45:10.776014Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1654594115089,
     "user": {
      "displayName": "Francisco Melo",
      "userId": "00306568718420504230"
     },
     "user_tz": -60
    },
    "id": "w3UYdxDzCLAK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We now create our custom class\n",
    "class NamesDataset(datautils.Dataset):\n",
    "    def __init__(self, names_file):\n",
    "\n",
    "        # We load the data from the csv file\n",
    "        name_data = pd.read_csv(names_file)\n",
    "        \n",
    "        # We create a list to store the input and output pairs\n",
    "        self.samples = []\n",
    "\n",
    "        # We run through the data in the Dataframe and fill in both lists\n",
    "        for idx in range(len(name_data)):\n",
    "            name  = input_encoding(name_data['Name'][idx])\n",
    "            label = torch.tensor([LABELS.index(name_data['Label'][idx])])\n",
    "\n",
    "            self.samples += [(name, label)]\n",
    "\n",
    "        shuffle(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WasZteXkFu7e"
   },
   "source": [
    "We can now create a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T10:45:10.787234Z",
     "iopub.status.busy": "2023-05-26T10:45:10.786374Z",
     "iopub.status.idle": "2023-05-26T10:45:11.302367Z",
     "shell.execute_reply": "2023-05-26T10:45:11.301408Z",
     "shell.execute_reply.started": "2023-05-26T10:45:10.787206Z"
    },
    "executionInfo": {
     "elapsed": 588,
     "status": "ok",
     "timestamp": 1654594115673,
     "user": {
      "displayName": "Francisco Melo",
      "userId": "00306568718420504230"
     },
     "user_tz": -60
    },
    "id": "nscUGkkeF24a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set = NamesDataset('train_data.csv')\n",
    "valid_set = NamesDataset('test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QV1Dg9UQc89U"
   },
   "source": [
    "We can plot loss and accuracy during training, and see how they evolved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urW11NfjOhow"
   },
   "source": [
    "## Text generation using LSTM\n",
    "\n",
    "We are now going to perform a slightly more sophisticated task on the same data we used before. In particular, we will now _generate_ names from the different countries that follow the same pattern found in the names in the dataset.\n",
    "\n",
    "To that purpose, we need to perform a little more ground work. We slightly reformulate the `NamesDataset` class to now have as input a sequence of symbols and a category, and as output another sequence of symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T10:45:11.303366Z",
     "iopub.status.busy": "2023-05-26T10:45:11.303174Z",
     "iopub.status.idle": "2023-05-26T10:45:11.313417Z",
     "shell.execute_reply": "2023-05-26T10:45:11.312111Z",
     "shell.execute_reply.started": "2023-05-26T10:45:11.303349Z"
    },
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1654594634328,
     "user": {
      "displayName": "Francisco Melo",
      "userId": "00306568718420504230"
     },
     "user_tz": -60
    },
    "id": "J5vNj2fTd8gC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We now create our custom class\n",
    "class NamesDataset(datautils.Dataset):\n",
    "    def __init__(self, names_file):\n",
    "\n",
    "        # We load the data from the csv file\n",
    "        name_data = pd.read_csv(names_file)\n",
    "\n",
    "        # We create a list to store the input and output pairs\n",
    "        self.samples = []\n",
    "\n",
    "        # We run through the data in the Dataframe and fill in both lists\n",
    "        for idx in range(len(name_data)):\n",
    "            name  = name_data['Name'][idx]\n",
    "            label = name_data['Label'][idx]\n",
    "\n",
    "            self.samples += [(name, label)]\n",
    "\n",
    "        shuffle(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "\n",
    "        name = item[0]\n",
    "        label = item[1]\n",
    "        \n",
    "        # Since we're using an embedding layer, we no longer \n",
    "        # use one-hot encoding, but store only the index        \n",
    "        input_tensor  = torch.tensor([[LETTERS.index(x)] for x in name], dtype=torch.long)\n",
    "        label_tensor  = label_encoding(label)\n",
    "        target_tensor = torch.tensor([LETTERS.index(x) for x in name[1:]] + [LETTERS.index('<eos>')], dtype=torch.long)\n",
    "        \n",
    "        item_dict = {\"label\": label,\n",
    "                     \"name\": name,\n",
    "                     \"label_tensor\": label_tensor,\n",
    "                     \"input_tensor\": input_tensor,\n",
    "                     \"target_tensor\": target_tensor}        \n",
    "        \n",
    "        return item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T10:45:11.315265Z",
     "iopub.status.busy": "2023-05-26T10:45:11.314737Z",
     "iopub.status.idle": "2023-05-26T10:45:11.398709Z",
     "shell.execute_reply": "2023-05-26T10:45:11.397508Z",
     "shell.execute_reply.started": "2023-05-26T10:45:11.315198Z"
    },
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1654594637040,
     "user": {
      "displayName": "Francisco Melo",
      "userId": "00306568718420504230"
     },
     "user_tz": -60
    },
    "id": "PGhCnMV2gm8p",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set = NamesDataset('train_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T10:45:11.400927Z",
     "iopub.status.busy": "2023-05-26T10:45:11.400702Z",
     "iopub.status.idle": "2023-05-26T10:45:11.411181Z",
     "shell.execute_reply": "2023-05-26T10:45:11.410095Z",
     "shell.execute_reply.started": "2023-05-26T10:45:11.400909Z"
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1654594638828,
     "user": {
      "displayName": "Francisco Melo",
      "userId": "00306568718420504230"
     },
     "user_tz": -60
    },
    "id": "KcOhIvGjiJrK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMNetwork(nn.Module):\n",
    "    def __init__(self, input_size, n_labels, embedding_size, hidden_size, output_size, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First, an embedding layer is used to convert the one-hot encoding \n",
    "        # into a feature vector\n",
    "        self.i2f_layer = nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "        # We then create an LSTM layer\n",
    "        self.f2h_layer = nn.LSTM(embedding_size + n_labels, hidden_size, 1)\n",
    "\n",
    "        # Then, a linear layer that turns the LSTM hidden state into an \n",
    "        # output prediction\n",
    "        self.h2o_layer = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # We include a dropout layer for the embedding\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # We add initialization parameters for the hidden state and cell\n",
    "        self.hidden_init = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "        self.cell_init   = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "\n",
    "    def single_pass(self, letter_tensor, label_tensor, hidden, cell):\n",
    "        # Compute embedding\n",
    "        f = self.dropout(self.i2f_layer(letter_tensor))\n",
    "        \n",
    "        # Peform lstm pass\n",
    "        o, (h, c) = self.f2h_layer(torch.cat((f, label_tensor), 1), (hidden, cell))\n",
    "        \n",
    "        # Compute output\n",
    "        o = self.h2o_layer(o)\n",
    "\n",
    "        return o, h, c\n",
    "\n",
    "    def forward(self, input):\n",
    "        name = input['input_tensor']\n",
    "        label = input['label_tensor']\n",
    "\n",
    "        h = self.hidden_init\n",
    "        c = self.cell_init\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for letter in name:\n",
    "            out, h, c = self.single_pass(letter, label, h, c)\n",
    "            outputs += [out]\n",
    "\n",
    "        # We return all outputs\n",
    "        return torch.cat(outputs)\n",
    "\n",
    "    def sample(self, label, start_letter, max_length=20):\n",
    "        ''' We will use this function to generate names given a label.'''\n",
    "            \n",
    "        # During sampling, we store no gradient information\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            label_tensor = label_encoding(label)\n",
    "            letter_tensor = torch.tensor([LETTERS.index(start_letter)], dtype=torch.long)\n",
    "          \n",
    "            h = self.hidden_init\n",
    "            c = self.cell_init\n",
    "\n",
    "            output = [start_letter]\n",
    "\n",
    "            for i in range(max_length):\n",
    "                o, h, c = self.single_pass(letter_tensor, label_tensor, h, c)\n",
    "                \n",
    "                _, next_idx = torch.max(o, dim=1)\n",
    "                next_letter = LETTERS[next_idx]\n",
    "\n",
    "                if next_letter == \"<eos>\":\n",
    "                    break\n",
    "                else:\n",
    "                    output += [next_letter]\n",
    "                    letter_tensor = torch.tensor([next_idx], dtype=torch.long)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "        return ''.join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBmkEjns0ovu"
   },
   "source": [
    "We can now run the training routine. Note, however, that the output is now a sequence of distributions, one per symbol, so the computation of the loss must be different. For this reason, we do not use the `train_network` function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-05-26T10:45:11.412682Z",
     "iopub.status.busy": "2023-05-26T10:45:11.412310Z",
     "iopub.status.idle": "2023-05-26T11:23:48.536939Z",
     "shell.execute_reply": "2023-05-26T11:23:48.532599Z",
     "shell.execute_reply.started": "2023-05-26T10:45:11.412662Z"
    },
    "executionInfo": {
     "elapsed": 1330728,
     "status": "ok",
     "timestamp": 1654595973791,
     "user": {
      "displayName": "Francisco Melo",
      "userId": "00306568718420504230"
     },
     "user_tz": -60
    },
    "id": "pMeGDeiMaqv3",
    "outputId": "8e57e871-6ec1-49a7-b22d-2eadb430333c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated name in Portuguese: PItjjkuuuFoiii.....\n",
      "Generated name in French: ,yy.....\n",
      "Generated name in Spanish: CkPouuFKoiiZoii......\n",
      "Generated name in English: SuuFK\n",
      "Generated name in English: OuFKoii....\n",
      "Generated name in Spanish: Q-PstjjkuuuFKoiiZoii.\n",
      "Generated name in English: OuFKoii....\n",
      "Generated name in Portuguese: EFoii......\n",
      "Generated name in Spanish: YlkuuuFKoiiZoii......\n",
      "Generated name in English: FoppUsjji....\n",
      "\n",
      "- Training epoch: 0 -\n",
      "Training loss: 13.7971\n",
      "\n",
      "- Training epoch: 1 -\n",
      "Training loss: 12.1272\n",
      "\n",
      "- Training epoch: 2 -\n",
      "Training loss: 11.4020\n",
      "\n",
      "- Training epoch: 3 -\n",
      "Training loss: 10.8731\n",
      "\n",
      "- Training epoch: 4 -\n",
      "Training loss: 10.4619\n",
      "\n",
      "- Training epoch: 5 -\n",
      "Training loss: 10.0617\n",
      "\n",
      "- Training epoch: 6 -\n",
      "Training loss: 9.6972\n",
      "\n",
      "- Training epoch: 7 -\n",
      "Training loss: 9.3869\n",
      "\n",
      "- Training epoch: 8 -\n",
      "Training loss: 9.1246\n",
      "\n",
      "- Training epoch: 9 -\n",
      "Training loss: 8.8961\n",
      "\n",
      "- Training epoch: 10 -\n",
      "Training loss: 8.6407\n",
      "\n",
      "- Training epoch: 11 -\n",
      "Training loss: 8.4324\n",
      "\n",
      "- Training epoch: 12 -\n",
      "Training loss: 8.2768\n",
      "\n",
      "- Training epoch: 13 -\n",
      "Training loss: 8.1117\n",
      "\n",
      "- Training epoch: 14 -\n",
      "Training loss: 7.9634\n",
      "\n",
      "- Training epoch: 15 -\n",
      "Training loss: 7.8224\n",
      "\n",
      "- Training epoch: 16 -\n",
      "Training loss: 7.6767\n",
      "\n",
      "- Training epoch: 17 -\n",
      "Training loss: 7.5987\n",
      "\n",
      "- Training epoch: 18 -\n",
      "Training loss: 7.5142\n",
      "\n",
      "- Training epoch: 19 -\n",
      "Training loss: 7.4528\n",
      "\n",
      "- Training epoch: 20 -\n",
      "Training loss: 7.3479\n",
      "\n",
      "- Training epoch: 21 -\n",
      "Training loss: 7.2671\n",
      "\n",
      "- Training epoch: 22 -\n",
      "Training loss: 7.2478\n",
      "\n",
      "- Training epoch: 23 -\n",
      "Training loss: 7.1803\n",
      "\n",
      "- Training epoch: 24 -\n",
      "Training loss: 7.1457\n",
      "\n",
      "- Training epoch: 25 -\n",
      "Training loss: 7.0715\n",
      "\n",
      "- Training epoch: 26 -\n",
      "Training loss: 7.0785\n",
      "\n",
      "- Training epoch: 27 -\n",
      "Training loss: 6.9944\n",
      "\n",
      "- Training epoch: 28 -\n",
      "Training loss: 6.9795\n",
      "\n",
      "- Training epoch: 29 -\n",
      "Training loss: 6.9391\n",
      "\n",
      "- Training epoch: 30 -\n",
      "Training loss: 6.9274\n",
      "\n",
      "- Training epoch: 31 -\n",
      "Training loss: 6.9193\n",
      "\n",
      "- Training epoch: 32 -\n",
      "Training loss: 6.8467\n",
      "\n",
      "- Training epoch: 33 -\n",
      "Training loss: 6.8332\n",
      "\n",
      "- Training epoch: 34 -\n",
      "Training loss: 6.8031\n",
      "\n",
      "- Training epoch: 35 -\n",
      "Training loss: 6.8062\n",
      "\n",
      "- Training epoch: 36 -\n",
      "Training loss: 6.8033\n",
      "\n",
      "- Training epoch: 37 -\n",
      "Training loss: 6.7395\n",
      "\n",
      "- Training epoch: 38 -\n",
      "Training loss: 6.7518\n",
      "\n",
      "- Training epoch: 39 -\n",
      "Training loss: 6.7246\n"
     ]
    }
   ],
   "source": [
    "# We create an instance of our LSTM network.\n",
    "lstm_net = LSTMNetwork(len(LETTERS), len(LABELS), 128, 256, len(LETTERS), dropout=0.3)\n",
    "\n",
    "# Before training, let's check how well the network generates names\n",
    "for i in range(10):\n",
    "    label = np.random.choice(LABELS)\n",
    "    letter = np.random.choice(LETTERS[:-1]).upper()\n",
    "    \n",
    "    print(f'Generated name in {label}:', lstm_net.sample(label, letter))\n",
    "\n",
    "# We use the cross-entropy loss _per letter_\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# ... and Adam as the optimizer\n",
    "optim = torch.optim.Adam(lstm_net.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "num_epochs = 40\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    print('\\n- Training epoch: %i -' % ep)\n",
    "\n",
    "    # We use auxiliary variables to keep track of loss and accuracy within \n",
    "    # an epoch\n",
    "    running_loss = 0.\n",
    "\n",
    "    for sample in train_set:\n",
    "        target = sample['target_tensor']\n",
    "\n",
    "        # We zero-out the gradient\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # We initialize the loss to 0\n",
    "        l = 0\n",
    "\n",
    "        # Compute output\n",
    "        outputs = lstm_net(sample)\n",
    "\n",
    "        # We now compute the loss for each letter in the input name, given the \n",
    "        # target.\n",
    "        for i in range(len(target)):\n",
    "            l += loss(outputs[i], target[i])\n",
    "\n",
    "        # Compute gradient\n",
    "        l.backward()\n",
    "            \n",
    "        # Perform optimization step\n",
    "        optim.step()\n",
    "\n",
    "        # Update total running loss. We account for the number of points \n",
    "        # in the batch\n",
    "        running_loss += l.item()\n",
    "             \n",
    "    train_losses += [running_loss / len(train_set)]\n",
    "\n",
    "    print(f'Training loss: {train_losses[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJ9rsz_jfGqg"
   },
   "source": [
    "Now that we trained out network, let us see how well it is able to generate names given a label/language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T12:02:04.468523Z",
     "iopub.status.busy": "2023-05-26T12:02:04.467418Z",
     "iopub.status.idle": "2023-05-26T12:02:04.557710Z",
     "shell.execute_reply": "2023-05-26T12:02:04.555982Z",
     "shell.execute_reply.started": "2023-05-26T12:02:04.468489Z"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "aborted",
     "timestamp": 1654594125505,
     "user": {
      "displayName": "Francisco Melo",
      "userId": "00306568718420504230"
     },
     "user_tz": -60
    },
    "id": "mX1__QUYfW4s",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated name in English: Newby\n",
      "Generated name in English: Ellison\n",
      "Generated name in French: Irelane\n",
      "Generated name in Portuguese: Xuerra\n",
      "Generated name in French: Gagnier\n",
      "Generated name in Portuguese: Lobo\n",
      "Generated name in Portuguese: D'cruz\n",
      "Generated name in Spanish: Yeardo\n",
      "Generated name in French: Kent\n",
      "Generated name in French: Newell\n",
      "Generated name in English: Stannard\n",
      "Generated name in Portuguese: Escarra\n",
      "Generated name in Portuguese: Lobo\n",
      "Generated name in Portuguese: Herrero\n",
      "Generated name in Spanish: Paredes\n",
      "Generated name in French: Fabian\n",
      "Generated name in Spanish: Ness\n",
      "Generated name in English: Macarthur\n",
      "Generated name in Spanish: Ferrer\n",
      "Generated name in French: Elvis\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    label = np.random.choice(LABELS)\n",
    "    letter = np.random.choice(list('ABCDEFGHIJKLMNOPQRSTUVWXYZ'))\n",
    "\n",
    "    print(f'Generated name in {label}:', lstm_net.sample(label, letter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T14:11:52.284548Z",
     "iopub.status.busy": "2023-05-26T14:11:52.284118Z",
     "iopub.status.idle": "2023-05-26T14:11:52.903514Z",
     "shell.execute_reply": "2023-05-26T14:11:52.901134Z",
     "shell.execute_reply.started": "2023-05-26T14:11:52.284525Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated name in English: Newby\n",
      "Generated name in English: Yarwood\n",
      "Generated name in English: Upton\n",
      "Generated name in English: Kenneth\n",
      "Generated name in English: Harrison\n",
      "Generated name in English: Godfrey\n",
      "Generated name in English: Farrel\n",
      "Generated name in English: Locke\n",
      "Generated name in English: Zaunton\n",
      "Generated name in English: Allcott\n",
      "Generated name in French: Kent\n",
      "Generated name in French: Elvis\n",
      "Generated name in French: Zaville\n",
      "Generated name in French: Martel\n",
      "Generated name in French: Paris\n",
      "Generated name in French: Zaville\n",
      "Generated name in French: Elvis\n",
      "Generated name in French: Martel\n",
      "Generated name in French: Newell\n",
      "Generated name in French: Oliver\n",
      "Generated name in Portuguese: Barros\n",
      "Generated name in Portuguese: Lobo\n",
      "Generated name in Portuguese: Quiros\n",
      "Generated name in Portuguese: Xuerra\n",
      "Generated name in Portuguese: Araullo\n",
      "Generated name in Portuguese: Kentera\n",
      "Generated name in Portuguese: Quiros\n",
      "Generated name in Portuguese: Paredes\n",
      "Generated name in Portuguese: Fabro\n",
      "Generated name in Portuguese: Villa\n",
      "Generated name in Spanish: Lobo\n",
      "Generated name in Spanish: Lobo\n",
      "Generated name in Spanish: Ness\n",
      "Generated name in Spanish: Zambrano\n",
      "Generated name in Spanish: Urbina\n",
      "Generated name in Spanish: Araya\n",
      "Generated name in Spanish: Urbina\n",
      "Generated name in Spanish: Araya\n",
      "Generated name in Spanish: Ferrer\n",
      "Generated name in Spanish: Herrera\n"
     ]
    }
   ],
   "source": [
    "for label in LABELS:\n",
    "    for i in range(10):\n",
    "        letter = np.random.choice(list('ABCDEFGHIJKLMNOPQRSTUVWXYZ'))\n",
    "        print(f'Generated name in {label}:', lstm_net.sample(label, letter))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOHHhena3Vq04n5OcUy/Wmn",
   "collapsed_sections": [],
   "name": "Lab3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
