{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8jx134dakEm"
   },
   "source": [
    "# Test LSTM in GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T09:20:16.766664Z",
     "iopub.status.busy": "2023-05-31T09:20:16.766461Z",
     "iopub.status.idle": "2023-05-31T09:20:20.579975Z",
     "shell.execute_reply": "2023-05-31T09:20:20.579189Z",
     "shell.execute_reply.started": "2023-05-31T09:20:16.766647Z"
    },
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1654594114799,
     "user": {
      "displayName": "Francisco Melo",
      "userId": "00306568718420504230"
     },
     "user_tz": -60
    },
    "id": "aWvGUgnDabis",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from random import shuffle, choice\n",
    "from timeit import default_timer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as datautils\n",
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T09:20:20.580880Z",
     "iopub.status.busy": "2023-05-31T09:20:20.580616Z",
     "iopub.status.idle": "2023-05-31T09:20:23.867781Z",
     "shell.execute_reply": "2023-05-31T09:20:23.866670Z",
     "shell.execute_reply.started": "2023-05-31T09:20:20.580862Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used to store tensor: cuda:0\n",
      "Device name: Quadro RTX 3000\n",
      "Allocated: 0.0 MB\n",
      "Cached:    2.0 MB\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(3, 4)\n",
    "# Let's now move our tensor to a GPU, if one is available\n",
    "if torch.cuda.is_available():\n",
    "    t = t.to('cuda')\n",
    "    print('Device used to store tensor:', t.device)\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**2,3), 'MB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**2,3), 'MB')\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    print('No GPU available.')\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T11:37:19.025247Z",
     "iopub.status.busy": "2023-05-30T11:37:19.024135Z",
     "iopub.status.idle": "2023-05-30T11:37:19.029768Z",
     "shell.execute_reply": "2023-05-30T11:37:19.028481Z",
     "shell.execute_reply.started": "2023-05-30T11:37:19.025226Z"
    },
    "tags": []
   },
   "source": [
    "## Dataset Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T09:20:23.869930Z",
     "iopub.status.busy": "2023-05-31T09:20:23.869154Z",
     "iopub.status.idle": "2023-05-31T09:20:23.878000Z",
     "shell.execute_reply": "2023-05-31T09:20:23.876287Z",
     "shell.execute_reply.started": "2023-05-31T09:20:23.869911Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1654594115089,
     "user": {
      "displayName": "Francisco Melo",
      "userId": "00306568718420504230"
     },
     "user_tz": -60
    },
    "id": "K9SCqr-7rMNq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "LABELS = ['English', 'French', 'Portuguese', 'Spanish']\n",
    "LETTERS = list(string.ascii_letters + \" .,;'-\") + ['<eos>']\n",
    "\n",
    "def input_encoding(input_str):\n",
    "    \"\"\" Receives a string as input and returns, as output, a Pytorch tensor\n",
    "        containing the one-hot encoding of the provided string. \"\"\"\n",
    "\n",
    "    one_hot_string = torch.zeros(len(input_str), 1, len(LETTERS), dtype=torch.long)\n",
    "    \n",
    "    for letter_idx in range(len(input_str)):\n",
    "        letter = input_str[letter_idx]\n",
    "        one_hot_string[letter_idx][0][LETTERS.index(letter)] = 1\n",
    "\n",
    "    return one_hot_string\n",
    "\n",
    "def label_encoding(output_str):\n",
    "    \"\"\" Receives a string as input and returns, as output, a Pytorch tensor\n",
    "        containing the one-hot encoding of the provided label string. \"\"\"\n",
    "\n",
    "    one_hot_label = torch.zeros(1, len(LABELS), dtype=torch.long)\n",
    "    label_idx = LABELS.index(output_str)\n",
    "    one_hot_label[0][label_idx] = 1\n",
    "\n",
    "    return one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T09:20:23.881836Z",
     "iopub.status.busy": "2023-05-31T09:20:23.881376Z",
     "iopub.status.idle": "2023-05-31T09:20:23.987679Z",
     "shell.execute_reply": "2023-05-31T09:20:23.986857Z",
     "shell.execute_reply.started": "2023-05-31T09:20:23.881795Z"
    },
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1654594634328,
     "user": {
      "displayName": "Francisco Melo",
      "userId": "00306568718420504230"
     },
     "user_tz": -60
    },
    "id": "J5vNj2fTd8gC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We now create our custom class\n",
    "class NamesDataset(datautils.Dataset):\n",
    "    def __init__(self, names_file):\n",
    "\n",
    "        # We load the data from the csv file\n",
    "        name_data = pd.read_csv(names_file)\n",
    "\n",
    "        # We create a list to store the input and output pairs\n",
    "        self.samples = []\n",
    "\n",
    "        # We run through the data in the Dataframe and fill in both lists\n",
    "        for idx in range(len(name_data)):\n",
    "            name  = name_data['Name'][idx]\n",
    "            label = name_data['Label'][idx]\n",
    "\n",
    "            self.samples += [(name, label)]\n",
    "\n",
    "        shuffle(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "\n",
    "        name = item[0]\n",
    "        label = item[1]\n",
    "        \n",
    "        # Since we're using an embedding layer, we no longer \n",
    "        # use one-hot encoding, but store only the index        \n",
    "        input_tensor  = torch.tensor([[LETTERS.index(x)] for x in name],\n",
    "                                     dtype=torch.long)\n",
    "        label_tensor  = label_encoding(label)\n",
    "        target_tensor = torch.tensor([LETTERS.index(x) for x in name[1:]] + [LETTERS.index('<eos>')],\n",
    "                                     dtype=torch.long)\n",
    "        \n",
    "        item_dict = {\"label\": label,\n",
    "                     \"name\": name,\n",
    "                     \"label_tensor\": label_tensor.to('cuda'),\n",
    "                     \"input_tensor\": input_tensor.to('cuda'),\n",
    "                     \"target_tensor\": target_tensor.to('cuda')}        \n",
    "\n",
    "        return item_dict\n",
    "    \n",
    "train_set = NamesDataset('train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urW11NfjOhow"
   },
   "source": [
    "## LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T09:20:23.988557Z",
     "iopub.status.busy": "2023-05-31T09:20:23.988364Z",
     "iopub.status.idle": "2023-05-31T09:20:23.998730Z",
     "shell.execute_reply": "2023-05-31T09:20:23.997635Z",
     "shell.execute_reply.started": "2023-05-31T09:20:23.988540Z"
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1654594638828,
     "user": {
      "displayName": "Francisco Melo",
      "userId": "00306568718420504230"
     },
     "user_tz": -60
    },
    "id": "KcOhIvGjiJrK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMNetwork(nn.Module):\n",
    "    def __init__(self, input_size, n_labels, embedding_size, hidden_size, output_size, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First, an embedding layer is used to convert the one-hot encoding\n",
    "        # into a feature vector\n",
    "        self.i2f_layer = nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "        # We then create an LSTM layer\n",
    "        self.f2h_layer = nn.LSTM(embedding_size + n_labels, hidden_size, 1)\n",
    "\n",
    "        # Then, a linear layer that turns the LSTM hidden state into an\n",
    "        # output prediction\n",
    "        self.h2o_layer = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # We include a dropout layer for the embedding\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # We add initialization parameters for the hidden state and cell\n",
    "        self.hidden_init = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "        self.cell_init   = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "        \n",
    "        # self.LETTERS = list(string.ascii_letters + \" .,;'-\") + ['<eos>']\n",
    "\n",
    "    def single_pass(self, letter_tensor, label_tensor, hidden, cell):\n",
    "        # Compute embedding\n",
    "        f = self.dropout(self.i2f_layer(letter_tensor))\n",
    "        \n",
    "        # Peform lstm pass\n",
    "        o, (h, c) = self.f2h_layer(torch.cat((f, label_tensor), 1), (hidden, cell))\n",
    "        \n",
    "        # Compute output\n",
    "        o = self.h2o_layer(o)\n",
    "\n",
    "        return o, h, c\n",
    "\n",
    "    def forward(self, input):\n",
    "        name = input['input_tensor']\n",
    "        label = input['label_tensor']\n",
    "\n",
    "        h = self.hidden_init\n",
    "        c = self.cell_init\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for letter in name:\n",
    "            out, h, c = self.single_pass(letter, label, h, c)\n",
    "            outputs += [out]\n",
    "\n",
    "        # We return all outputs\n",
    "        return torch.cat(outputs)\n",
    "\n",
    "    def sample(self, label, start_letter, max_length=20):\n",
    "        \"\"\" We will use this function to generate names given a label. \"\"\"\n",
    "            \n",
    "        # During sampling, we store no gradient information\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            label_tensor = label_encoding(label).to(device)\n",
    "            letter_tensor = torch.tensor([LETTERS.index(start_letter)], dtype=torch.long).to(device)\n",
    "          \n",
    "            h = self.hidden_init\n",
    "            c = self.cell_init\n",
    "\n",
    "            output = [start_letter]\n",
    "\n",
    "            for i in range(max_length):\n",
    "                o, h, c = self.single_pass(letter_tensor, label_tensor, h, c)\n",
    "                \n",
    "                _, next_idx = torch.max(o, dim=1)\n",
    "                next_letter = LETTERS[next_idx]\n",
    "\n",
    "                if next_letter == \"<eos>\":\n",
    "                    break\n",
    "                else:\n",
    "                    output += [next_letter]\n",
    "                    letter_tensor = torch.tensor([next_idx], dtype=torch.long).to(device)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "        return ''.join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-05-31T09:20:24.000988Z",
     "iopub.status.busy": "2023-05-31T09:20:24.000372Z",
     "iopub.status.idle": "2023-05-31T09:28:34.310004Z",
     "shell.execute_reply": "2023-05-31T09:28:34.308965Z",
     "shell.execute_reply.started": "2023-05-31T09:20:24.000960Z"
    },
    "executionInfo": {
     "elapsed": 1330728,
     "status": "ok",
     "timestamp": 1654595973791,
     "user": {
      "displayName": "Francisco Melo",
      "userId": "00306568718420504230"
     },
     "user_tz": -60
    },
    "id": "pMeGDeiMaqv3",
    "outputId": "8e57e871-6ec1-49a7-b22d-2eadb430333c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Training epoch: 0 -\n",
      "Training loss: 14.1207\n",
      "Epoch time: 23.89944734300002 (s)\n",
      "\n",
      "- Training epoch: 1 -\n",
      "Training loss: 12.3142\n",
      "Epoch time: 31.151397010000096 (s)\n",
      "\n",
      "- Training epoch: 2 -\n",
      "Training loss: 11.5496\n",
      "Epoch time: 25.409626731000003 (s)\n",
      "\n",
      "- Training epoch: 3 -\n",
      "Training loss: 10.9259\n",
      "Epoch time: 25.688770825000006 (s)\n",
      "\n",
      "- Training epoch: 4 -\n",
      "Training loss: 10.4256\n",
      "Epoch time: 24.138412084000038 (s)\n",
      "\n",
      "- Training epoch: 5 -\n",
      "Training loss: 10.0019\n",
      "Epoch time: 24.089317722000033 (s)\n",
      "\n",
      "- Training epoch: 6 -\n",
      "Training loss: 9.6226\n",
      "Epoch time: 23.88697811599991 (s)\n",
      "\n",
      "- Training epoch: 7 -\n",
      "Training loss: 9.2569\n",
      "Epoch time: 24.034181203999992 (s)\n",
      "\n",
      "- Training epoch: 8 -\n",
      "Training loss: 8.9615\n",
      "Epoch time: 24.386107373000073 (s)\n",
      "\n",
      "- Training epoch: 9 -\n",
      "Training loss: 8.6488\n",
      "Epoch time: 23.97241528400002 (s)\n",
      "\n",
      "- Training epoch: 10 -\n",
      "Training loss: 8.3880\n",
      "Epoch time: 23.955219046000025 (s)\n",
      "\n",
      "- Training epoch: 11 -\n",
      "Training loss: 8.1302\n",
      "Epoch time: 23.53554447099998 (s)\n",
      "\n",
      "- Training epoch: 12 -\n",
      "Training loss: 7.9474\n",
      "Epoch time: 23.951244489000032 (s)\n",
      "\n",
      "- Training epoch: 13 -\n",
      "Training loss: 7.8408\n",
      "Epoch time: 23.707475593000026 (s)\n",
      "\n",
      "- Training epoch: 14 -\n",
      "Training loss: 7.6532\n",
      "Epoch time: 23.68888144799996 (s)\n",
      "\n",
      "- Training epoch: 15 -\n",
      "Training loss: 7.5086\n",
      "Epoch time: 23.695417356999997 (s)\n",
      "\n",
      "- Training epoch: 16 -\n",
      "Training loss: 7.4090\n",
      "Epoch time: 24.018635273999962 (s)\n",
      "\n",
      "- Training epoch: 17 -\n",
      "Training loss: 7.3072\n",
      "Epoch time: 24.11095913400004 (s)\n",
      "\n",
      "- Training epoch: 18 -\n",
      "Training loss: 7.2145\n",
      "Epoch time: 23.76903035600003 (s)\n",
      "\n",
      "- Training epoch: 19 -\n",
      "Training loss: 7.1369\n",
      "Epoch time: 23.469810797000036 (s)\n"
     ]
    }
   ],
   "source": [
    "# We create an instance of our LSTM network.\n",
    "lstm_net = LSTMNetwork(len(LETTERS), len(LABELS), 128, 256, len(LETTERS), dropout=0.3).to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(lstm_net.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    epoch_start_time = default_timer()\n",
    "    print('\\n- Training epoch: %i -' % ep)\n",
    "\n",
    "    # We use auxiliary variables to keep track of loss and accuracy within \n",
    "    # an epoch\n",
    "    running_loss = 0.\n",
    "\n",
    "    for sample in train_set:\n",
    "        target = sample['target_tensor']\n",
    "\n",
    "        # We zero-out the gradient\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # We initialize the loss to 0\n",
    "        l = 0\n",
    "        \n",
    "        # Compute output\n",
    "        outputs = lstm_net(sample)\n",
    "\n",
    "        # We now compute the loss for each letter in the input name, given the target\n",
    "        for i in range(len(target)):\n",
    "            l += loss(outputs[i], target[i])\n",
    "\n",
    "        # Compute gradient\n",
    "        l.backward()\n",
    "        # Perform optimization step\n",
    "        optim.step()\n",
    "        # Update total running loss. We account for the number of points in the batch\n",
    "        running_loss += l.item()\n",
    "             \n",
    "    train_losses += [running_loss / len(train_set)]\n",
    "\n",
    "    print(f'Training loss: {train_losses[-1]:.4f}')\n",
    "    epoch_end_time = default_timer()\n",
    "    print(f'Epoch time: {epoch_end_time - epoch_start_time} (s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T09:28:34.311573Z",
     "iopub.status.busy": "2023-05-31T09:28:34.311290Z",
     "iopub.status.idle": "2023-05-31T09:28:34.317234Z",
     "shell.execute_reply": "2023-05-31T09:28:34.316136Z",
     "shell.execute_reply.started": "2023-05-31T09:28:34.311555Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_init torch.Size([1, 256])\n",
      "cell_init torch.Size([1, 256])\n",
      "i2f_layer.weight torch.Size([59, 128])\n",
      "f2h_layer.weight_ih_l0 torch.Size([1024, 132])\n",
      "f2h_layer.weight_hh_l0 torch.Size([1024, 256])\n",
      "f2h_layer.bias_ih_l0 torch.Size([1024])\n",
      "f2h_layer.bias_hh_l0 torch.Size([1024])\n",
      "h2o_layer.weight torch.Size([59, 256])\n",
      "h2o_layer.bias torch.Size([59])\n",
      "Total parameters: 422587\n"
     ]
    }
   ],
   "source": [
    "for name, param in lstm_net.named_parameters():\n",
    "    print(name, param.shape)\n",
    "    \n",
    "print('Total parameters:', sum(p.numel() for p in lstm_net.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T09:28:34.319426Z",
     "iopub.status.busy": "2023-05-31T09:28:34.318997Z",
     "iopub.status.idle": "2023-05-31T09:28:34.503747Z",
     "shell.execute_reply": "2023-05-31T09:28:34.502850Z",
     "shell.execute_reply.started": "2023-05-31T09:28:34.319402Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated name in English: Kenward\n",
      "Generated name in English: Harris\n",
      "Generated name in English: Thornhill\n",
      "Generated name in English: Thornhill\n",
      "Generated name in English: Ellerby\n",
      "Generated name in English: Thornhill\n",
      "Generated name in English: Mcneil\n",
      "Generated name in English: Alden\n",
      "Generated name in English: Yeardsley\n",
      "Generated name in English: Alden\n",
      "Generated name in French: Abascal\n",
      "Generated name in French: Whitelaw\n",
      "Generated name in French: Fabian\n",
      "Generated name in French: Zamontaine\n",
      "Generated name in French: Beller\n",
      "Generated name in French: De la fuente\n",
      "Generated name in French: Quiros\n",
      "Generated name in French: Abascal\n",
      "Generated name in French: Villeneuve\n",
      "Generated name in French: Xarter\n",
      "Generated name in Portuguese: Pares\n",
      "Generated name in Portuguese: De la cruz\n",
      "Generated name in Portuguese: Castro\n",
      "Generated name in Portuguese: Wood\n",
      "Generated name in Portuguese: Tomas\n",
      "Generated name in Portuguese: Belo\n",
      "Generated name in Portuguese: Albuquerque\n",
      "Generated name in Portuguese: Fabra\n",
      "Generated name in Portuguese: Norman\n",
      "Generated name in Portuguese: Ingledo\n",
      "Generated name in Spanish: Martinez\n",
      "Generated name in Spanish: Levaquerque\n",
      "Generated name in Spanish: Castillion\n",
      "Generated name in Spanish: Gaspar\n",
      "Generated name in Spanish: Escarra\n",
      "Generated name in Spanish: Oliver\n",
      "Generated name in Spanish: Bello\n",
      "Generated name in Spanish: Kendez\n",
      "Generated name in Spanish: Palomo\n",
      "Generated name in Spanish: Xartina\n"
     ]
    }
   ],
   "source": [
    "for label in LABELS:\n",
    "    for i in range(10):\n",
    "        letter = np.random.choice(list('ABCDEFGHIJKLMNOPQRSTUVWXYZ'))\n",
    "        print(f'Generated name in {label}:', lstm_net.sample(label, letter))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOHHhena3Vq04n5OcUy/Wmn",
   "collapsed_sections": [],
   "name": "Lab3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
